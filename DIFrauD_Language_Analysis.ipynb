{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DIFrauD Multilingual Data Quality Assessment\n",
    "## Analyzing Language Diversity Impact on Classification Performance\n",
    "\n",
    "**Course:** COSC 4371 Security Analytics - Fall 2025  \n",
    "**Team Members:** Joseph Mascardo, Niket Gupta  \n",
    "\n",
    "---\n",
    "\n",
    "### Project Objective\n",
    "Investigate whether all samples in the DIFrauD dataset are in English, analyze language distribution by class and domain, and study the impact on classification performance.\n",
    "\n",
    "### Research Questions\n",
    "1. What is the language distribution across classes and domains in DIFrauD?\n",
    "2. How does removing non-English samples affect classifier performance?\n",
    "3. Do transformer-based models handle multilingual content better than traditional ML?\n",
    "\n",
    "---\n",
    "\n",
    "## External Sources and References\n",
    "\n",
    "### Dataset\n",
    "- **DIFrauD Dataset**: https://huggingface.co/datasets/difraud/difraud\n",
    "- **Citation**: Boumber, D., et al. (2024). \"Domain-Agnostic Adapter Architecture for Deception Detection.\" LREC-COLING 2024.\n",
    "\n",
    "### Libraries Used\n",
    "- **langdetect**: https://pypi.org/project/langdetect/ - Language detection (port of Google's language-detection)\n",
    "- **datasets**: https://huggingface.co/docs/datasets/ - HuggingFace datasets library\n",
    "- **transformers**: https://huggingface.co/docs/transformers/ - HuggingFace transformers for DistilBERT\n",
    "- **scikit-learn**: https://scikit-learn.org/ - Traditional ML classifiers\n",
    "- **pandas/numpy**: Data processing\n",
    "- **matplotlib/seaborn**: Visualizations\n",
    "\n",
    "### Key References\n",
    "- Conneau, A., et al. (2020). \"Unsupervised cross-lingual representation learning at scale.\" ACL 2020.\n",
    "- Devlin, J., et al. (2019). \"BERT: Pre-training of deep bidirectional transformers.\" NAACL 2019.\n",
    "- Verma, R. M., et al. (2019). \"Data quality for security challenges.\" ACM CCS 2019."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Environment Setup and Imports\n",
    "\n",
    "**Steps taken:**\n",
    "1. Install required packages\n",
    "2. Import necessary libraries\n",
    "3. Set random seeds for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run once)\n",
    "# Source: Standard pip installation\n",
    "!pip install -q datasets langdetect transformers torch scikit-learn pandas numpy matplotlib seaborn tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "\n",
    "# Dataset loading - Source: https://huggingface.co/docs/datasets/\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "\n",
    "# Language detection - Source: https://pypi.org/project/langdetect/\n",
    "from langdetect import detect, detect_langs, LangDetectException\n",
    "\n",
    "# ML libraries - Source: https://scikit-learn.org/\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, f1_score, \n",
    "    precision_score, recall_score, accuracy_score,\n",
    "    balanced_accuracy_score\n",
    ")\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Deep Learning - Source: https://huggingface.co/docs/transformers/\n",
    "import torch\n",
    "from transformers import (\n",
    "    DistilBertTokenizer, DistilBertForSequenceClassification,\n",
    "    Trainer, TrainingArguments, EarlyStoppingCallback\n",
    ")\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Progress tracking\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Statistical testing\n",
    "from scipy import stats\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Load DIFrauD Dataset\n",
    "\n",
    "**Steps taken:**\n",
    "1. Load all 7 domains from HuggingFace\n",
    "2. Combine train, validation, and test splits\n",
    "3. Create unified DataFrame with domain labels\n",
    "\n",
    "**Dataset Source:** https://huggingface.co/datasets/difraud/difraud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define all domains in DIFrauD dataset\n",
    "DOMAINS = [\n",
    "    'fake_news',\n",
    "    'job_scams', \n",
    "    'phishing',\n",
    "    'political_statements',\n",
    "    'product_reviews',\n",
    "    'sms',\n",
    "    'twitter_rumours'\n",
    "]\n",
    "\n",
    "def load_difraud_dataset():\n",
    "    \"\"\"\n",
    "    Load all domains from DIFrauD dataset by directly reading JSONL files.\n",
    "    Source: HuggingFace datasets library\n",
    "    Dataset: https://huggingface.co/datasets/difraud/difraud\n",
    "    \n",
    "    Note: The dataset uses legacy loading scripts no longer supported by datasets library,\n",
    "    so we load directly from the JSONL files using data_files parameter.\n",
    "    \"\"\"\n",
    "    all_data = []\n",
    "    \n",
    "    for domain in tqdm(DOMAINS, desc=\"Loading domains\"):\n",
    "        try:\n",
    "            # Load directly from JSONL files using data_files parameter\n",
    "            # This bypasses the deprecated loading script\n",
    "            base_url = f\"https://huggingface.co/datasets/difraud/difraud/resolve/main/{domain}\"\n",
    "            \n",
    "            dataset = load_dataset(\n",
    "                'json',\n",
    "                data_files={\n",
    "                    'train': f\"{base_url}/train.jsonl\",\n",
    "                    'validation': f\"{base_url}/validation.jsonl\",\n",
    "                    'test': f\"{base_url}/test.jsonl\"\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            # Combine all splits\n",
    "            for split in ['train', 'validation', 'test']:\n",
    "                if split in dataset:\n",
    "                    df_split = dataset[split].to_pandas()\n",
    "                    df_split['domain'] = domain\n",
    "                    df_split['split'] = split\n",
    "                    all_data.append(df_split)\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {domain}: {e}\")\n",
    "    \n",
    "    # Combine all data\n",
    "    if len(all_data) == 0:\n",
    "        raise ValueError(\"No data was loaded. Check dataset availability and internet connection.\")\n",
    "    \n",
    "    df = pd.concat(all_data, ignore_index=True)\n",
    "    return df\n",
    "\n",
    "# Load the dataset\n",
    "print(\"Loading DIFrauD dataset from HuggingFace...\")\n",
    "print(\"(Downloading JSONL files directly - this may take a few minutes)\\n\")\n",
    "df = load_difraud_dataset()\n",
    "\n",
    "print(f\"\\nDataset loaded successfully!\")\n",
    "print(f\"Total samples: {len(df):,}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset overview\n",
    "print(\"=\"*60)\n",
    "print(\"DATASET OVERVIEW\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n--- Samples by Domain ---\")\n",
    "domain_counts = df.groupby('domain').agg({\n",
    "    'text': 'count',\n",
    "    'label': ['sum', 'mean']\n",
    "}).round(3)\n",
    "domain_counts.columns = ['Total', 'Deceptive', 'Deceptive_Ratio']\n",
    "domain_counts['Non-Deceptive'] = domain_counts['Total'] - domain_counts['Deceptive']\n",
    "print(domain_counts)\n",
    "\n",
    "print(\"\\n--- Overall Class Distribution ---\")\n",
    "print(f\"Deceptive (label=1): {df['label'].sum():,} ({df['label'].mean()*100:.2f}%)\")\n",
    "print(f\"Non-Deceptive (label=0): {(df['label']==0).sum():,} ({(1-df['label'].mean())*100:.2f}%)\")\n",
    "\n",
    "print(\"\\n--- Sample Text Lengths ---\")\n",
    "df['text_length'] = df['text'].str.len()\n",
    "print(df.groupby('domain')['text_length'].describe().round(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Language Detection Pipeline\n",
    "\n",
    "**Steps taken:**\n",
    "1. Implement language detection using `langdetect` library\n",
    "2. Handle edge cases (short texts, detection errors)\n",
    "3. Apply to all samples and record detected languages\n",
    "\n",
    "**Source:** langdetect library - https://pypi.org/project/langdetect/  \n",
    "**Note:** langdetect is a port of Google's language-detection library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_language_safe(text, min_length=20):\n",
    "    \"\"\"\n",
    "    Safely detect language of text with error handling.\n",
    "    \n",
    "    Source: langdetect library (https://pypi.org/project/langdetect/)\n",
    "    \n",
    "    Parameters:\n",
    "    - text: Input text string\n",
    "    - min_length: Minimum text length for reliable detection\n",
    "    \n",
    "    Returns:\n",
    "    - Tuple of (detected_language_code, confidence_score)\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or len(text.strip()) < min_length:\n",
    "        return ('unknown', 0.0)\n",
    "    \n",
    "    try:\n",
    "        # Get language probabilities\n",
    "        langs = detect_langs(text)\n",
    "        # Return top language and its probability\n",
    "        top_lang = langs[0]\n",
    "        return (top_lang.lang, top_lang.prob)\n",
    "    except LangDetectException:\n",
    "        return ('unknown', 0.0)\n",
    "    except Exception as e:\n",
    "        return ('error', 0.0)\n",
    "\n",
    "# Test the function\n",
    "test_texts = [\n",
    "    \"This is a test message in English.\",\n",
    "    \"Ceci est un message de test en français.\",\n",
    "    \"Pathaya enketa maraikara pa\",  # From SMS dataset (Tamil)\n",
    "    \"短文本\"  # Short Chinese text\n",
    "]\n",
    "\n",
    "print(\"Language Detection Test:\")\n",
    "for text in test_texts:\n",
    "    lang, conf = detect_language_safe(text)\n",
    "    print(f\"  '{text[:40]}...' -> {lang} (conf: {conf:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply language detection to entire dataset\n",
    "print(\"Detecting languages for all samples...\")\n",
    "print(\"(This may take several minutes)\\n\")\n",
    "\n",
    "# Apply with progress bar\n",
    "tqdm.pandas(desc=\"Detecting languages\")\n",
    "language_results = df['text'].progress_apply(detect_language_safe)\n",
    "\n",
    "# Extract language codes and confidence scores\n",
    "df['detected_language'] = language_results.apply(lambda x: x[0])\n",
    "df['language_confidence'] = language_results.apply(lambda x: x[1])\n",
    "\n",
    "print(\"\\nLanguage detection completed!\")\n",
    "print(f\"Unique languages detected: {df['detected_language'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Language Distribution Analysis\n",
    "\n",
    "**Steps taken:**\n",
    "1. Calculate language distribution overall\n",
    "2. Analyze by class (deceptive vs non-deceptive)\n",
    "3. Analyze by domain\n",
    "4. Perform chi-square tests for significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall language distribution\n",
    "print(\"=\"*60)\n",
    "print(\"OVERALL LANGUAGE DISTRIBUTION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "lang_counts = df['detected_language'].value_counts()\n",
    "lang_percentages = df['detected_language'].value_counts(normalize=True) * 100\n",
    "\n",
    "lang_summary = pd.DataFrame({\n",
    "    'Count': lang_counts,\n",
    "    'Percentage': lang_percentages.round(2)\n",
    "})\n",
    "print(lang_summary.head(15))\n",
    "\n",
    "# English vs Non-English\n",
    "df['is_english'] = df['detected_language'] == 'en'\n",
    "print(f\"\\n--- English vs Non-English ---\")\n",
    "print(f\"English samples: {df['is_english'].sum():,} ({df['is_english'].mean()*100:.2f}%)\")\n",
    "print(f\"Non-English samples: {(~df['is_english']).sum():,} ({(~df['is_english']).mean()*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Language distribution by CLASS (deceptive vs non-deceptive)\n",
    "print(\"=\"*60)\n",
    "print(\"LANGUAGE DISTRIBUTION BY CLASS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "class_lang_dist = pd.crosstab(\n",
    "    df['label'].map({0: 'Non-Deceptive', 1: 'Deceptive'}),\n",
    "    df['is_english'].map({True: 'English', False: 'Non-English'}),\n",
    "    margins=True\n",
    ")\n",
    "print(\"\\nCounts:\")\n",
    "print(class_lang_dist)\n",
    "\n",
    "# Percentages within each class\n",
    "class_lang_pct = pd.crosstab(\n",
    "    df['label'].map({0: 'Non-Deceptive', 1: 'Deceptive'}),\n",
    "    df['is_english'].map({True: 'English', False: 'Non-English'}),\n",
    "    normalize='index'\n",
    ") * 100\n",
    "print(\"\\nPercentages (within each class):\")\n",
    "print(class_lang_pct.round(2))\n",
    "\n",
    "# Chi-square test for class vs language\n",
    "contingency = pd.crosstab(df['label'], df['is_english'])\n",
    "chi2, p_value, dof, expected = stats.chi2_contingency(contingency)\n",
    "print(f\"\\nChi-square test (Class vs Language):\")\n",
    "print(f\"  Chi-square statistic: {chi2:.4f}\")\n",
    "print(f\"  p-value: {p_value:.4e}\")\n",
    "print(f\"  Significant (p < 0.05): {'Yes' if p_value < 0.05 else 'No'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Language distribution by DOMAIN\n",
    "print(\"=\"*60)\n",
    "print(\"LANGUAGE DISTRIBUTION BY DOMAIN\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "domain_lang_analysis = []\n",
    "\n",
    "for domain in DOMAINS:\n",
    "    domain_df = df[df['domain'] == domain]\n",
    "    \n",
    "    total = len(domain_df)\n",
    "    english = domain_df['is_english'].sum()\n",
    "    non_english = total - english\n",
    "    \n",
    "    # Top non-English languages\n",
    "    non_eng_langs = domain_df[~domain_df['is_english']]['detected_language'].value_counts().head(3)\n",
    "    top_non_eng = ', '.join([f\"{lang}({cnt})\" for lang, cnt in non_eng_langs.items()])\n",
    "    \n",
    "    domain_lang_analysis.append({\n",
    "        'Domain': domain,\n",
    "        'Total': total,\n",
    "        'English': english,\n",
    "        'Non-English': non_english,\n",
    "        'English %': (english/total*100),\n",
    "        'Non-English %': (non_english/total*100),\n",
    "        'Top Non-English Languages': top_non_eng\n",
    "    })\n",
    "\n",
    "domain_lang_df = pd.DataFrame(domain_lang_analysis)\n",
    "print(domain_lang_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed breakdown: Language distribution by Domain AND Class\n",
    "print(\"=\"*60)\n",
    "print(\"LANGUAGE DISTRIBUTION BY DOMAIN AND CLASS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "detailed_analysis = []\n",
    "\n",
    "for domain in DOMAINS:\n",
    "    for label in [0, 1]:\n",
    "        subset = df[(df['domain'] == domain) & (df['label'] == label)]\n",
    "        \n",
    "        if len(subset) == 0:\n",
    "            continue\n",
    "            \n",
    "        total = len(subset)\n",
    "        english = subset['is_english'].sum()\n",
    "        \n",
    "        # Get top 5 detected languages\n",
    "        lang_dist = subset['detected_language'].value_counts().head(5).to_dict()\n",
    "        \n",
    "        detailed_analysis.append({\n",
    "            'Domain': domain,\n",
    "            'Class': 'Deceptive' if label == 1 else 'Non-Deceptive',\n",
    "            'Total': total,\n",
    "            'English': english,\n",
    "            'English %': round(english/total*100, 2),\n",
    "            'Non-English': total - english,\n",
    "            'Non-English %': round((total-english)/total*100, 2),\n",
    "            'Languages': lang_dist\n",
    "        })\n",
    "\n",
    "detailed_df = pd.DataFrame(detailed_analysis)\n",
    "print(detailed_df[['Domain', 'Class', 'Total', 'English', 'English %', 'Non-English', 'Non-English %']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: Overall language distribution (top 10)\n",
    "ax1 = axes[0, 0]\n",
    "top_langs = df['detected_language'].value_counts().head(10)\n",
    "colors = ['green' if lang == 'en' else 'coral' for lang in top_langs.index]\n",
    "top_langs.plot(kind='bar', ax=ax1, color=colors)\n",
    "ax1.set_title('Top 10 Detected Languages', fontsize=12)\n",
    "ax1.set_xlabel('Language Code')\n",
    "ax1.set_ylabel('Count')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Plot 2: English vs Non-English by domain\n",
    "ax2 = axes[0, 1]\n",
    "domain_lang_pivot = df.groupby('domain')['is_english'].agg(['sum', 'count'])\n",
    "domain_lang_pivot['non_english'] = domain_lang_pivot['count'] - domain_lang_pivot['sum']\n",
    "domain_lang_pivot[['sum', 'non_english']].plot(kind='bar', stacked=True, ax=ax2, \n",
    "                                                color=['green', 'coral'])\n",
    "ax2.set_title('English vs Non-English by Domain', fontsize=12)\n",
    "ax2.set_xlabel('Domain')\n",
    "ax2.set_ylabel('Count')\n",
    "ax2.legend(['English', 'Non-English'])\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Plot 3: Non-English percentage by domain\n",
    "ax3 = axes[1, 0]\n",
    "non_eng_pct = domain_lang_df.set_index('Domain')['Non-English %']\n",
    "non_eng_pct.plot(kind='bar', ax=ax3, color='coral')\n",
    "ax3.set_title('Non-English Percentage by Domain', fontsize=12)\n",
    "ax3.set_xlabel('Domain')\n",
    "ax3.set_ylabel('Non-English %')\n",
    "ax3.tick_params(axis='x', rotation=45)\n",
    "ax3.axhline(y=non_eng_pct.mean(), color='red', linestyle='--', label=f'Mean: {non_eng_pct.mean():.1f}%')\n",
    "ax3.legend()\n",
    "\n",
    "# Plot 4: Language distribution by class\n",
    "ax4 = axes[1, 1]\n",
    "class_lang_pct.plot(kind='bar', ax=ax4, color=['green', 'coral'])\n",
    "ax4.set_title('Language Distribution by Class', fontsize=12)\n",
    "ax4.set_xlabel('Class')\n",
    "ax4.set_ylabel('Percentage')\n",
    "ax4.legend(['English', 'Non-English'])\n",
    "ax4.tick_params(axis='x', rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('language_distribution_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nVisualization saved as 'language_distribution_analysis.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Create Dataset Splits (English-only vs Full)\n",
    "\n",
    "**Steps taken:**\n",
    "1. Create filtered English-only dataset\n",
    "2. Create full multilingual dataset\n",
    "3. Ensure consistent train/test splits for fair comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create English-only and Full datasets\n",
    "print(\"Creating dataset versions...\\n\")\n",
    "\n",
    "# Full dataset (all languages)\n",
    "df_full = df.copy()\n",
    "\n",
    "# English-only dataset\n",
    "df_english = df[df['is_english'] == True].copy()\n",
    "\n",
    "print(f\"Full dataset: {len(df_full):,} samples\")\n",
    "print(f\"English-only dataset: {len(df_english):,} samples\")\n",
    "print(f\"Samples removed: {len(df_full) - len(df_english):,} ({(1 - len(df_english)/len(df_full))*100:.2f}%)\")\n",
    "\n",
    "# Compare class distribution\n",
    "print(\"\\n--- Class Distribution Comparison ---\")\n",
    "print(f\"Full - Deceptive: {df_full['label'].mean()*100:.2f}%\")\n",
    "print(f\"English-only - Deceptive: {df_english['label'].mean()*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_train_test_data(df, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Prepare stratified train/test splits.\n",
    "    Uses stratification to handle class imbalance.\n",
    "    \n",
    "    Source: scikit-learn train_test_split\n",
    "    https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "    \"\"\"\n",
    "    X = df['text'].values\n",
    "    y = df['label'].values\n",
    "    domains = df['domain'].values\n",
    "    \n",
    "    X_train, X_test, y_train, y_test, domains_train, domains_test = train_test_split(\n",
    "        X, y, domains,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state,\n",
    "        stratify=y\n",
    "    )\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, domains_train, domains_test\n",
    "\n",
    "# Prepare data for both versions\n",
    "print(\"Preparing train/test splits...\\n\")\n",
    "\n",
    "# Full dataset\n",
    "X_train_full, X_test_full, y_train_full, y_test_full, domains_train_full, domains_test_full = \\\n",
    "    prepare_train_test_data(df_full)\n",
    "\n",
    "# English-only dataset\n",
    "X_train_eng, X_test_eng, y_train_eng, y_test_eng, domains_train_eng, domains_test_eng = \\\n",
    "    prepare_train_test_data(df_english)\n",
    "\n",
    "print(\"Full Dataset:\")\n",
    "print(f\"  Train: {len(X_train_full):,} | Test: {len(X_test_full):,}\")\n",
    "print(f\"  Train class dist: {np.mean(y_train_full)*100:.2f}% deceptive\")\n",
    "\n",
    "print(\"\\nEnglish-only Dataset:\")\n",
    "print(f\"  Train: {len(X_train_eng):,} | Test: {len(X_test_eng):,}\")\n",
    "print(f\"  Train class dist: {np.mean(y_train_eng)*100:.2f}% deceptive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Traditional ML Classifiers (Random Forest & SVM)\n",
    "\n",
    "**Steps taken:**\n",
    "1. Create TF-IDF features\n",
    "2. Train Random Forest and SVM classifiers\n",
    "3. Evaluate on both dataset versions\n",
    "4. Use F1-score as primary metric (suitable for imbalanced data)\n",
    "\n",
    "**Source:** scikit-learn - https://scikit-learn.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tfidf_features(X_train, X_test, max_features=10000):\n",
    "    \"\"\"\n",
    "    Create TF-IDF features from text data.\n",
    "    \n",
    "    Source: scikit-learn TfidfVectorizer\n",
    "    https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "    \"\"\"\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        max_features=max_features,\n",
    "        ngram_range=(1, 2),  # Unigrams and bigrams\n",
    "        min_df=2,           # Minimum document frequency\n",
    "        max_df=0.95,        # Maximum document frequency\n",
    "        sublinear_tf=True   # Apply sublinear tf scaling\n",
    "    )\n",
    "    \n",
    "    X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "    X_test_tfidf = vectorizer.transform(X_test)\n",
    "    \n",
    "    return X_train_tfidf, X_test_tfidf, vectorizer\n",
    "\n",
    "print(\"Creating TF-IDF features...\\n\")\n",
    "\n",
    "# Full dataset features\n",
    "X_train_full_tfidf, X_test_full_tfidf, vectorizer_full = \\\n",
    "    create_tfidf_features(X_train_full, X_test_full)\n",
    "print(f\"Full dataset - TF-IDF shape: {X_train_full_tfidf.shape}\")\n",
    "\n",
    "# English-only features\n",
    "X_train_eng_tfidf, X_test_eng_tfidf, vectorizer_eng = \\\n",
    "    create_tfidf_features(X_train_eng, X_test_eng)\n",
    "print(f\"English-only - TF-IDF shape: {X_train_eng_tfidf.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_classifier(clf, X_train, X_test, y_train, y_test, clf_name, dataset_name):\n",
    "    \"\"\"\n",
    "    Train classifier and return evaluation metrics.\n",
    "    \n",
    "    Uses metrics suitable for imbalanced datasets:\n",
    "    - F1-Score (weighted and macro)\n",
    "    - Balanced Accuracy\n",
    "    - Precision and Recall\n",
    "    \n",
    "    Source: scikit-learn metrics\n",
    "    \"\"\"\n",
    "    print(f\"\\nTraining {clf_name} on {dataset_name}...\")\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        'Classifier': clf_name,\n",
    "        'Dataset': dataset_name,\n",
    "        'Accuracy': accuracy_score(y_test, y_pred),\n",
    "        'Balanced_Accuracy': balanced_accuracy_score(y_test, y_pred),\n",
    "        'F1_Weighted': f1_score(y_test, y_pred, average='weighted'),\n",
    "        'F1_Macro': f1_score(y_test, y_pred, average='macro'),\n",
    "        'Precision_Weighted': precision_score(y_test, y_pred, average='weighted'),\n",
    "        'Recall_Weighted': recall_score(y_test, y_pred, average='weighted')\n",
    "    }\n",
    "    \n",
    "    print(f\"  F1 (weighted): {metrics['F1_Weighted']:.4f}\")\n",
    "    print(f\"  F1 (macro): {metrics['F1_Macro']:.4f}\")\n",
    "    print(f\"  Balanced Accuracy: {metrics['Balanced_Accuracy']:.4f}\")\n",
    "    \n",
    "    return metrics, y_pred, clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest\n",
    "# Source: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"RANDOM FOREST CLASSIFIER\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "rf_results = []\n",
    "\n",
    "# Random Forest on Full Dataset\n",
    "rf_full = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=None,\n",
    "    min_samples_split=2,\n",
    "    class_weight='balanced',  # Handle class imbalance\n",
    "    random_state=SEED,\n",
    "    n_jobs=-1\n",
    ")\n",
    "metrics_rf_full, pred_rf_full, _ = train_and_evaluate_classifier(\n",
    "    rf_full, X_train_full_tfidf, X_test_full_tfidf,\n",
    "    y_train_full, y_test_full,\n",
    "    'Random Forest', 'Full (Multilingual)'\n",
    ")\n",
    "rf_results.append(metrics_rf_full)\n",
    "\n",
    "# Random Forest on English-only Dataset\n",
    "rf_eng = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=None,\n",
    "    min_samples_split=2,\n",
    "    class_weight='balanced',\n",
    "    random_state=SEED,\n",
    "    n_jobs=-1\n",
    ")\n",
    "metrics_rf_eng, pred_rf_eng, _ = train_and_evaluate_classifier(\n",
    "    rf_eng, X_train_eng_tfidf, X_test_eng_tfidf,\n",
    "    y_train_eng, y_test_eng,\n",
    "    'Random Forest', 'English-only'\n",
    ")\n",
    "rf_results.append(metrics_rf_eng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SVM (Support Vector Machine) using LinearSVC\n",
    "# Source: https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html\n",
    "# Note: LinearSVC is faster than SVC with kernel='linear' for large sparse datasets like TF-IDF\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"SVM CLASSIFIER (LinearSVC)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "svm_results = []\n",
    "\n",
    "# LinearSVC on Full Dataset\n",
    "svm_full = LinearSVC(\n",
    "    C=1.0,\n",
    "    class_weight='balanced',\n",
    "    max_iter=10000,\n",
    "    random_state=SEED\n",
    ")\n",
    "metrics_svm_full, pred_svm_full, _ = train_and_evaluate_classifier(\n",
    "    svm_full, X_train_full_tfidf, X_test_full_tfidf,\n",
    "    y_train_full, y_test_full,\n",
    "    'SVM (LinearSVC)', 'Full (Multilingual)'\n",
    ")\n",
    "svm_results.append(metrics_svm_full)\n",
    "\n",
    "# LinearSVC on English-only Dataset\n",
    "svm_eng = LinearSVC(\n",
    "    C=1.0,\n",
    "    class_weight='balanced',\n",
    "    max_iter=10000,\n",
    "    random_state=SEED\n",
    ")\n",
    "metrics_svm_eng, pred_svm_eng, _ = train_and_evaluate_classifier(\n",
    "    svm_eng, X_train_eng_tfidf, X_test_eng_tfidf,\n",
    "    y_train_eng, y_test_eng,\n",
    "    'SVM (LinearSVC)', 'English-only'\n",
    ")\n",
    "svm_results.append(metrics_svm_eng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Transformer-Based Classifier (DistilBERT)\n",
    "\n",
    "**Steps taken:**\n",
    "1. Load pretrained DistilBERT model and tokenizer\n",
    "2. Fine-tune on both dataset versions\n",
    "3. Evaluate performance\n",
    "\n",
    "**Source:** HuggingFace Transformers - https://huggingface.co/docs/transformers/  \n",
    "**Model:** distilbert-base-uncased - https://huggingface.co/distilbert-base-uncased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DistilBERT Dataset Class\n",
    "class FraudDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Custom PyTorch Dataset for fraud detection.\n",
    "    Source: PyTorch Dataset API\n",
    "    \"\"\"\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=256):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Compute metrics for HuggingFace Trainer.\n",
    "    Uses metrics suitable for imbalanced data.\n",
    "    \"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy_score(labels, predictions),\n",
    "        'balanced_accuracy': balanced_accuracy_score(labels, predictions),\n",
    "        'f1_weighted': f1_score(labels, predictions, average='weighted'),\n",
    "        'f1_macro': f1_score(labels, predictions, average='macro'),\n",
    "        'precision': precision_score(labels, predictions, average='weighted'),\n",
    "        'recall': recall_score(labels, predictions, average='weighted')\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_distilbert(X_train, X_test, y_train, y_test, dataset_name, epochs=3, batch_size=16):\n",
    "    \"\"\"\n",
    "    Train DistilBERT classifier.\n",
    "    \n",
    "    Source: HuggingFace Transformers\n",
    "    Model: distilbert-base-uncased\n",
    "    https://huggingface.co/distilbert-base-uncased\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training DistilBERT on {dataset_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Load tokenizer and model\n",
    "    model_name = 'distilbert-base-uncased'\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "    model = DistilBertForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=2\n",
    "    )\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = FraudDataset(X_train, y_train, tokenizer)\n",
    "    test_dataset = FraudDataset(X_test, y_test, tokenizer)\n",
    "    \n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=f'./results_{dataset_name.replace(\" \", \"_\")}',\n",
    "        num_train_epochs=epochs,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        warmup_steps=500,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir='./logs',\n",
    "        logging_steps=100,\n",
    "        eval_strategy='epoch',\n",
    "        save_strategy='epoch',\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model='f1_weighted',\n",
    "        seed=SEED\n",
    "    )\n",
    "    \n",
    "    # Initialize trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=test_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    "    )\n",
    "    \n",
    "    # Train\n",
    "    trainer.train()\n",
    "    \n",
    "    # Evaluate\n",
    "    eval_results = trainer.evaluate()\n",
    "    \n",
    "    # Get predictions for detailed metrics\n",
    "    predictions = trainer.predict(test_dataset)\n",
    "    y_pred = np.argmax(predictions.predictions, axis=1)\n",
    "    \n",
    "    metrics = {\n",
    "        'Classifier': 'DistilBERT',\n",
    "        'Dataset': dataset_name,\n",
    "        'Accuracy': eval_results['eval_accuracy'],\n",
    "        'Balanced_Accuracy': eval_results['eval_balanced_accuracy'],\n",
    "        'F1_Weighted': eval_results['eval_f1_weighted'],\n",
    "        'F1_Macro': eval_results['eval_f1_macro'],\n",
    "        'Precision_Weighted': eval_results['eval_precision'],\n",
    "        'Recall_Weighted': eval_results['eval_recall']\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nResults for {dataset_name}:\")\n",
    "    print(f\"  F1 (weighted): {metrics['F1_Weighted']:.4f}\")\n",
    "    print(f\"  F1 (macro): {metrics['F1_Macro']:.4f}\")\n",
    "    print(f\"  Balanced Accuracy: {metrics['Balanced_Accuracy']:.4f}\")\n",
    "    \n",
    "    return metrics, y_pred, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train DistilBERT on both datasets\n",
    "# Note: This may take significant time depending on GPU availability\n",
    "\n",
    "distilbert_results = []\n",
    "\n",
    "# Check if GPU is available\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Sample size for faster training (optional - remove for full training)\n",
    "# Comment out these lines for full dataset training\n",
    "SAMPLE_SIZE = 5000  # Use smaller sample for demonstration\n",
    "print(f\"\\nNote: Using sample of {SAMPLE_SIZE} for demonstration.\")\n",
    "print(\"Remove SAMPLE_SIZE limit for full training.\\n\")\n",
    "\n",
    "# Sample data\n",
    "np.random.seed(SEED)\n",
    "sample_idx_full = np.random.choice(len(X_train_full), min(SAMPLE_SIZE, len(X_train_full)), replace=False)\n",
    "sample_idx_eng = np.random.choice(len(X_train_eng), min(SAMPLE_SIZE, len(X_train_eng)), replace=False)\n",
    "\n",
    "X_train_full_sample = X_train_full[sample_idx_full]\n",
    "y_train_full_sample = y_train_full[sample_idx_full]\n",
    "\n",
    "X_train_eng_sample = X_train_eng[sample_idx_eng]\n",
    "y_train_eng_sample = y_train_eng[sample_idx_eng]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on Full Dataset\n",
    "metrics_bert_full, pred_bert_full, model_full = train_distilbert(\n",
    "    X_train_full_sample, X_test_full[:1000],  # Smaller test set for speed\n",
    "    y_train_full_sample, y_test_full[:1000],\n",
    "    'Full (Multilingual)',\n",
    "    epochs=2,\n",
    "    batch_size=16\n",
    ")\n",
    "distilbert_results.append(metrics_bert_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train on English-only Dataset\n",
    "metrics_bert_eng, pred_bert_eng, model_eng = train_distilbert(\n",
    "    X_train_eng_sample, X_test_eng[:1000],\n",
    "    y_train_eng_sample, y_test_eng[:1000],\n",
    "    'English-only',\n",
    "    epochs=2,\n",
    "    batch_size=16\n",
    ")\n",
    "distilbert_results.append(metrics_bert_eng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Results Comparison and Analysis\n",
    "\n",
    "**Steps taken:**\n",
    "1. Compile all results\n",
    "2. Calculate domain-wise performance\n",
    "3. Compute aggregate metrics (mean and weighted)\n",
    "4. Statistical significance testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile all results\n",
    "all_results = rf_results + svm_results + distilbert_results\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"OVERALL RESULTS COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate performance difference\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PERFORMANCE DIFFERENCE (English-only vs Full)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for classifier in ['Random Forest', 'SVM', 'DistilBERT']:\n",
    "    clf_results = results_df[results_df['Classifier'] == classifier]\n",
    "    \n",
    "    if len(clf_results) < 2:\n",
    "        continue\n",
    "        \n",
    "    full_f1 = clf_results[clf_results['Dataset'].str.contains('Full')]['F1_Weighted'].values[0]\n",
    "    eng_f1 = clf_results[clf_results['Dataset'].str.contains('English')]['F1_Weighted'].values[0]\n",
    "    \n",
    "    diff = eng_f1 - full_f1\n",
    "    pct_change = (diff / full_f1) * 100\n",
    "    \n",
    "    print(f\"\\n{classifier}:\")\n",
    "    print(f\"  Full dataset F1: {full_f1:.4f}\")\n",
    "    print(f\"  English-only F1: {eng_f1:.4f}\")\n",
    "    print(f\"  Difference: {diff:+.4f} ({pct_change:+.2f}%)\")\n",
    "    print(f\"  Impact: {'Improved' if diff > 0 else 'Decreased'} with English-only data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain-wise performance analysis\n",
    "# Note: This requires per-domain evaluation which we'll compute here\n",
    "\n",
    "def evaluate_by_domain(y_true, y_pred, domains):\n",
    "    \"\"\"\n",
    "    Calculate performance metrics for each domain.\n",
    "    \"\"\"\n",
    "    domain_metrics = []\n",
    "    \n",
    "    for domain in DOMAINS:\n",
    "        mask = domains == domain\n",
    "        if mask.sum() == 0:\n",
    "            continue\n",
    "            \n",
    "        y_true_domain = y_true[mask]\n",
    "        y_pred_domain = y_pred[mask]\n",
    "        \n",
    "        domain_metrics.append({\n",
    "            'Domain': domain,\n",
    "            'Samples': mask.sum(),\n",
    "            'Accuracy': accuracy_score(y_true_domain, y_pred_domain),\n",
    "            'F1_Weighted': f1_score(y_true_domain, y_pred_domain, average='weighted', zero_division=0),\n",
    "            'F1_Macro': f1_score(y_true_domain, y_pred_domain, average='macro', zero_division=0)\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(domain_metrics)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"DOMAIN-WISE PERFORMANCE (Random Forest - Full Dataset)\")\n",
    "print(\"=\"*60)\n",
    "domain_perf_full = evaluate_by_domain(y_test_full, pred_rf_full, domains_test_full)\n",
    "print(domain_perf_full.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DOMAIN-WISE PERFORMANCE (Random Forest - English-only)\")\n",
    "print(\"=\"*60)\n",
    "domain_perf_eng = evaluate_by_domain(y_test_eng, pred_rf_eng, domains_test_eng)\n",
    "print(domain_perf_eng.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate metrics (Mean and Weighted)\n",
    "print(\"=\"*60)\n",
    "print(\"AGGREGATE PERFORMANCE METRICS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Mean performance across domains\n",
    "print(\"\\n--- Mean Performance (unweighted average across domains) ---\")\n",
    "print(f\"Full Dataset - Mean F1: {domain_perf_full['F1_Weighted'].mean():.4f}\")\n",
    "print(f\"English-only - Mean F1: {domain_perf_eng['F1_Weighted'].mean():.4f}\")\n",
    "\n",
    "# Weighted performance (weighted by number of samples)\n",
    "print(\"\\n--- Weighted Performance (weighted by domain size) ---\")\n",
    "weighted_f1_full = np.average(\n",
    "    domain_perf_full['F1_Weighted'], \n",
    "    weights=domain_perf_full['Samples']\n",
    ")\n",
    "weighted_f1_eng = np.average(\n",
    "    domain_perf_eng['F1_Weighted'], \n",
    "    weights=domain_perf_eng['Samples']\n",
    ")\n",
    "print(f\"Full Dataset - Weighted F1: {weighted_f1_full:.4f}\")\n",
    "print(f\"English-only - Weighted F1: {weighted_f1_eng:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: F1 Score comparison by classifier\n",
    "ax1 = axes[0, 0]\n",
    "classifiers = results_df['Classifier'].unique()\n",
    "x = np.arange(len(classifiers))\n",
    "width = 0.35\n",
    "\n",
    "full_f1 = [results_df[(results_df['Classifier']==c) & (results_df['Dataset'].str.contains('Full'))]['F1_Weighted'].values[0] \n",
    "           if len(results_df[(results_df['Classifier']==c) & (results_df['Dataset'].str.contains('Full'))]) > 0 else 0\n",
    "           for c in classifiers]\n",
    "eng_f1 = [results_df[(results_df['Classifier']==c) & (results_df['Dataset'].str.contains('English'))]['F1_Weighted'].values[0]\n",
    "          if len(results_df[(results_df['Classifier']==c) & (results_df['Dataset'].str.contains('English'))]) > 0 else 0\n",
    "          for c in classifiers]\n",
    "\n",
    "bars1 = ax1.bar(x - width/2, full_f1, width, label='Full (Multilingual)', color='coral')\n",
    "bars2 = ax1.bar(x + width/2, eng_f1, width, label='English-only', color='green')\n",
    "ax1.set_ylabel('F1 Score (Weighted)')\n",
    "ax1.set_title('F1 Score by Classifier and Dataset')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(classifiers)\n",
    "ax1.legend()\n",
    "ax1.set_ylim(0, 1)\n",
    "\n",
    "# Plot 2: Domain-wise F1 comparison\n",
    "ax2 = axes[0, 1]\n",
    "x = np.arange(len(DOMAINS))\n",
    "ax2.bar(x - width/2, domain_perf_full['F1_Weighted'], width, label='Full', color='coral')\n",
    "ax2.bar(x + width/2, domain_perf_eng['F1_Weighted'], width, label='English-only', color='green')\n",
    "ax2.set_ylabel('F1 Score (Weighted)')\n",
    "ax2.set_title('Domain-wise F1 Score (Random Forest)')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels([d.replace('_', '\\n') for d in DOMAINS], fontsize=8)\n",
    "ax2.legend()\n",
    "\n",
    "# Plot 3: Confusion Matrix (Full Dataset)\n",
    "ax3 = axes[1, 0]\n",
    "cm_full = confusion_matrix(y_test_full, pred_rf_full)\n",
    "sns.heatmap(cm_full, annot=True, fmt='d', cmap='Blues', ax=ax3)\n",
    "ax3.set_title('Confusion Matrix - Full Dataset (RF)')\n",
    "ax3.set_xlabel('Predicted')\n",
    "ax3.set_ylabel('Actual')\n",
    "\n",
    "# Plot 4: Confusion Matrix (English-only)\n",
    "ax4 = axes[1, 1]\n",
    "cm_eng = confusion_matrix(y_test_eng, pred_rf_eng)\n",
    "sns.heatmap(cm_eng, annot=True, fmt='d', cmap='Greens', ax=ax4)\n",
    "ax4.set_title('Confusion Matrix - English-only (RF)')\n",
    "ax4.set_xlabel('Predicted')\n",
    "ax4.set_ylabel('Actual')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('classification_results.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nVisualization saved as 'classification_results.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Summary and Conclusions\n",
    "\n",
    "### Key Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summary report\n",
    "print(\"=\"*80)\n",
    "print(\"FINAL SUMMARY REPORT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n### Dataset Analysis ###\")\n",
    "print(f\"Total samples analyzed: {len(df):,}\")\n",
    "print(f\"English samples: {df['is_english'].sum():,} ({df['is_english'].mean()*100:.2f}%)\")\n",
    "print(f\"Non-English samples: {(~df['is_english']).sum():,} ({(~df['is_english']).mean()*100:.2f}%)\")\n",
    "print(f\"Unique languages detected: {df['detected_language'].nunique()}\")\n",
    "\n",
    "print(\"\\n### Language Distribution by Domain ###\")\n",
    "print(domain_lang_df[['Domain', 'Total', 'Non-English', 'Non-English %']].to_string(index=False))\n",
    "\n",
    "print(\"\\n### Classification Performance Summary ###\")\n",
    "print(results_df[['Classifier', 'Dataset', 'F1_Weighted', 'Balanced_Accuracy']].to_string(index=False))\n",
    "\n",
    "print(\"\\n### Hypothesis Testing Results ###\")\n",
    "print(\"H1 (Data Composition): \", end=\"\")\n",
    "non_eng_pct = (~df['is_english']).mean() * 100\n",
    "if non_eng_pct > 1:\n",
    "    print(f\"SUPPORTED - {non_eng_pct:.2f}% non-English content found\")\n",
    "else:\n",
    "    print(f\"NOT SUPPORTED - Only {non_eng_pct:.2f}% non-English content\")\n",
    "\n",
    "print(\"H2 (Performance Impact): \", end=\"\")\n",
    "# Compare best F1 scores\n",
    "if len(results_df) > 0:\n",
    "    full_best = results_df[results_df['Dataset'].str.contains('Full')]['F1_Weighted'].max()\n",
    "    eng_best = results_df[results_df['Dataset'].str.contains('English')]['F1_Weighted'].max()\n",
    "    if eng_best > full_best:\n",
    "        print(f\"SUPPORTED - English-only shows higher F1 ({eng_best:.4f} vs {full_best:.4f})\")\n",
    "    else:\n",
    "        print(f\"NOT SUPPORTED - Full dataset shows comparable/better F1 ({full_best:.4f} vs {eng_best:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to CSV\n",
    "results_df.to_csv('classification_results.csv', index=False)\n",
    "domain_lang_df.to_csv('language_distribution_by_domain.csv', index=False)\n",
    "\n",
    "# Save detailed language analysis\n",
    "df[['text', 'label', 'domain', 'detected_language', 'language_confidence', 'is_english']].to_csv(\n",
    "    'difraud_language_analysis.csv', index=False\n",
    ")\n",
    "\n",
    "print(\"\\nResults saved to:\")\n",
    "print(\"  - classification_results.csv\")\n",
    "print(\"  - language_distribution_by_domain.csv\")\n",
    "print(\"  - difraud_language_analysis.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## References and Sources\n",
    "\n",
    "### Dataset\n",
    "- **DIFrauD Dataset**: Boumber, D., et al. (2024). \"Domain-Agnostic Adapter Architecture for Deception Detection.\" LREC-COLING 2024. Available at: https://huggingface.co/datasets/difraud/difraud\n",
    "\n",
    "### Libraries and Code Sources\n",
    "- **langdetect**: Language detection library (port of Google's language-detection). https://pypi.org/project/langdetect/\n",
    "- **HuggingFace datasets**: Dataset loading library. https://huggingface.co/docs/datasets/\n",
    "- **HuggingFace transformers**: Transformer models (DistilBERT). https://huggingface.co/docs/transformers/\n",
    "- **scikit-learn**: ML classifiers (Random Forest, SVM) and metrics. https://scikit-learn.org/\n",
    "- **DistilBERT model**: distilbert-base-uncased. https://huggingface.co/distilbert-base-uncased\n",
    "\n",
    "### Academic References\n",
    "- Conneau, A., et al. (2020). \"Unsupervised cross-lingual representation learning at scale.\" ACL 2020.\n",
    "- Devlin, J., et al. (2019). \"BERT: Pre-training of deep bidirectional transformers.\" NAACL 2019.\n",
    "- Verma, R. M., et al. (2019). \"Data quality for security challenges.\" ACM CCS 2019.\n",
    "\n",
    "### Metrics Choice Justification\n",
    "- **F1-Score (Weighted)**: Used as primary metric due to class imbalance in DIFrauD dataset. Weighted F1 accounts for class distribution.\n",
    "- **F1-Score (Macro)**: Unweighted average across classes, useful for evaluating performance on minority class.\n",
    "- **Balanced Accuracy**: Accounts for class imbalance by averaging recall across classes.\n",
    "\n",
    "### Code Notes\n",
    "- All code in this notebook is original unless otherwise noted\n",
    "- API usage follows official documentation from respective libraries\n",
    "- Random seed (42) used for reproducibility"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
